---
title:  "Summary: TAP: Text-Aware Pre-training for Text-VQA and Text-Caption"
mathjax: true
layout: post
categories: 
      - Transformers
---

## Tóm tắt
- Mục tiêu phương pháp đọc và hiểu chữ trong hình.

- **Ba nhiệm vụ tiền huấn luyện:** TAP sử dụng ba nhiệm vụ để giúp mô hình học mối liên hệ giữa chữ trong ảnh, văn bản và hình ảnh:
    - Mô hình ngôn ngữ che dấu (MLM): nhiệm vụ này giúp mô hình học cách dự đoán các từ bị che khuất trong cả văn bản và chữ trong ảnh.
    - Đối sánh tương phản ảnh-ngôn ngữ (ITM): nhiệm vụ này giúp mô hình học cách phân biệt giữa cặp ảnh-ngôn ngữ khớp nhau và không khớp nhau. Chữ trong ảnh sẽ được sử dụng để tăng cường mối liên hệ giữa ảnh và văn bản.
    - Dự đoán vị trí tương đối (không gian) (RPP): nhiệm vụ này giúp mô hình học cách xác định vị trí tương đối của các từ trong văn bản và chữ trong ảnh so với các đối tượng trong ảnh.
## Tổng quan lí thuyết
### Về tác vụ hình ảnh liên quan scene-text
- Các nghiên cứu gần đây đã đề xuất nhiều kiến trúc mạng khác nhau để cải thiện hiệu năng của TextVQA và Text-Caption.
    - LoRRA mở rộng mô hình VQA Pythia bằng cách thêm nhánh chú ý cho công cụ nhận dạng ký tự quang học (OCR) để xử lý chữ trong ảnh.
    - Multi-modal Multi-Copy Mesh (M4C) cải thiện hiệu năng của TextVQA nhờ mô-đun kết hợp đa phương thức nền Transformer và mô-đun giải mã lựa chọn nhiều bước. Biến thể M4CCaptioner của M4C đạt hiệu suất tốt trên TextCaps khi loại bỏ đầu vào là câu hỏi.
    - SA-M4C cải tiến M4C bằng cách mã hóa các mối quan hệ giữa các vùng ảnh thành mask chú ý trong Transformer đa phương thức. Các nghiên cứu tương tự về mối quan hệ giữa các vùng ảnh cũng được thực hiện trong tác vụ Text-Caption.
- Mặc dù có các thiết kế mạng hiệu quả, tất cả các nghiên cứu trước đây đều chỉ tối ưu trực tiếp cho một tác vụ duy nhất là TextVQA hoặc Text-Caption.
- Tác giả cho rằng việc chỉ sử dụng một hàm mất mát cho câu trả lời/chú thích duy nhất có thể ảnh hưởng đến việc học biểu diễn liên kết và do đó hạn chế hiệu năng của TextVQA và Text-Caption.
### Về tác vụ hình ảnh
- Các phương pháp VLP thông thường bỏ qua việc xử lý chữ trong ảnh trong quá trình tiền huấn luyện, do đó chúng kém hiệu quả cho TextVQA và Text-Caption.
## Kĩ thuật đề xuất
### Text-aware pre-training tasks
- Trong giai đoạn tiền huấn luyện, đầu vào của khối融合 (hợp nhất - fusion) gồm các vector biểu diễn (embeddings) của:
    - K từ ngữ w
    - M vùng đối tượng vobj
    - N vùng chữ trong ảnh vocr
    - Một ký hiệu bắt đầu đặc biệt p0
- Trong vector biểu diễn từ ngữ, mỗi từ trong chuỗi ký tự mở rộng w = wq; wobj; wocr được mã hóa thành một vector đặc trưng,
    - wq là câu hỏi
    - wobj là các nhãn đối tượng được phát hiện
    - wocr là các từ được nhận dạng từ chữ trong ảnh
- **Cải thiện tổng hợp đa phương thức (multi-modal fusion)**
    - Dựa trên đặc trưng tổng hợp f = fw; fobj; focr; fp, TAP cải thiện việc tổng hợp đa phương thức bằng cách thực hiện các tác vụ tiền huấn luyện nhận biết chữ trong ảnh.
    - Các tác vụ tiền huấn luyện được đề xuất bao gồm hai phần, tập trung lần lượt vào việc kết hợp chữ trong ảnh vocr với từ ngữ w và đối tượng trực quan vobj.

#### Scene-text language pre-training tasks (Kết hợp text và OCR)
- Để kết hợp tốt hơn chữ trong ảnh vocr với các từ ngữ w, tác giả thiết kế hai tác vụ tiền huấn luyện ngôn ngữ - chữ trong ảnh dựa trên các tác vụ mô hình hóa ngôn ngữ che dấu (masked language modeling - MLM) và đối sánh ảnh-ngôn ngữ (đối chiếu) (image-text (contrastive) matching - ITM) trong VLP.
##### MLM
- Đối với MLM trên đầu vào văn bản mở rộng w = wq; wobj; wocr, tác giả sẽ che dấu ngẫu nhiên 15% các token văn bản trong w.
    - Các từ bị che wmask được thay thế bằng một token MASK đặc biệt 80% thời gian, 10% thời gian được thay bằng một từ ngẫu nhiên, và 10% thời gian được giữ nguyên.
- Nhiệm vụ MLM lấy đặc trưng tổng hợp tại vị trí bị che fmask w làm đầu vào và nhằm phục hồi lại từ bị che wmask bằng hai lớp kết nối đầy đủ.
  
##### ITM
- Đối với ITM, 50% thời gian văn bản w bị nhiễu loạn bằng cách thay thế các chuỗi phụ wq, wobj hoặc wocr bằng một chuỗi được chọn ngẫu nhiên từ một ảnh khác. Do đó, các từ văn bản bị nhiễu loạn w không được ghép nối với các vùng trực quan vobj và vocr.
- Nhiệm vụ ITM lấy đặc trưng chuỗi f0p làm đầu vào và nhằm dự đoán xem chuỗi đó có bị nhiễu loạn hay không.

#### Scene-text visual pre-training tasks  (Kết hợp hình ảnh và OCR)
- TAP giải quyết vấn đề bằng cách đưa thêm một tác vụ tiền huấn luyện về mối quan hệ vị trí.
- Tác vụ này được gọi là "dự đoán vị trí tương đối (relative position prediction - RPP)". 
[Paper](https://arxiv.org/abs/2012.04638])

------
## Q&A:
1. Mã hoá QH vùng ảnh thành mask trong SA-M4C. 
